# 위클리\_페이퍼\_02 : 데이터 전처리 방법들과 t-test

---

# 데이터 전처리란?

데이터 전처리는 **데이터를 사용하기에 알맞게 다듬는 과정이다.**

데이터를 날것 그대로 사용한다면, 분석 시 **성능과 안정성이 매우 떨어지기 때문.**

전처리 과정에서는 다음과 같은 단계들이 있다.

---

### 1. 결측치 처리 (빈칸 채우기)

#### 1-1. 제거하기

- **무시하고 빼버리기**
  - Ex: 학생 100명의 키·몸무게를 조사했는데, 5명은 몸무게를 안쓴 칸이 있는 상태
  - 이 5명을 제외시켜버리는 방법
  ```python
  df = df.dropna() # 빈칸 있는 행(사람) 삭제
  ```

#### 1-2. 채워넣기

- **가장 흔한 값으로 메우기(최빈값)**
  - Ex: 친구 10명 중 3명이 좋아하는 과일을 안 적은 상태.
  - 빈칸을 **가장 많이 나온 과일**로 채워 넣으면 자연스러워짐

```python
most_common = df['fruit'].mode()[0]   # 최빈값 구하기
df['fruit'] = df['fruit'].fillna(most_common)
```

- 평균값으로 채우기
  - 키나 몸무게처럼 **숫자라면 평균이나 중간값으로 채워 넣는다.**

```python
avg_weight = df['weight'].mean()      # 평균 구하기
df['weight'] = df['weight'].fillna(avg_weight)
```

### 2. 범주형 변수 인코딩 (글자 → 숫자로 바꾸기)

- 머신러닝 모델은 숫자만 이해함. 그래서 '서울', '부산', '대구' 같은 글자를 숫자로 바꿔야 함.

#### 2-1. 숫자 레이블 붙이기 (Label Encoding)

- 간단히 0, 1, 2... 번호를 붙이는 방법.
- Ex: '서울' → 0, '부산' → 1, '대구' → 2

```python
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['city_num'] = le.fit_transform(df['city'])
```

장점: 쉽고 빠르다.  
단점: 0, 1, 2 의미에 '순서'가 있다고 모델이 착각할 수 있다.

#### 2-2. 하나씩 표시하기 (One-Hot Encoding)

- Ex: 각 도시마다 "이 도시에 살아요?" 라는 별도 칸을 만들고, 맞으면 1, 아니면 0 을 표시함

  | 도시 | is\_서울 | is\_부산 | is\_대구 |
  | ---- | -------- | -------- | -------- |
  | 서울 | **1**    | 0        | 0        |
  | 부산 | 0        | **1**    | 0        |
  | 대구 | 0        | 0        | **1**    |

```python
df = pd.get_dummies(df, columns=['city'], prefix='is')
```

장점: 순서 착각 없음  
단점: 도시가 많아지면 칸이 너무 많아짐

### 3. 수치형 변수 스케일링 (숫자 크기 맞춰주기)

- 숫자 크기를 맟춰주는 이유
  - Ex: 몸무게는 보통 50~100kg, 나이는 10~70세, 월급은 200만~1000만원
  - 이렇게 숫자 단위가 제각각이면, 큰 숫자에 따라서 결과가 달라질 수 있음.
  - 그래서 숫자를 비슷한 범위를 맟춰줘야 함.

#### 3-1. 최소~최대 정규화 (Min-Max Scaling)

- 모든 숫자를 0~1사이로 바꾸는 방법.
- 예:

  - 몸무게 50kg → 0.0

  - 몸무게 75kg → 0.5

  - 몸무게 100kg → 1.0

```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df[['weight_scaled']] = scaler.fit_transform(df[['weight']])
```

장점: 범위가 딱 정해져 있어서 직관적임  
단점: 너무 큰 값(이상치)이 있으면 왜곡될 수 있음

#### 3-2. 표준화 (Standardization)

- 숫자를 평균은 0, 표준편차(흩어진 정도)는 1로 맟추는 방법
- Ex: 평균보다 크면 +숫자, 작으면 -숫자

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[['weight_std']] = scaler.fit_transform(df[['weight']])
```

장점: 이상치에 조금 더 강함  
단점: 직관적으로 보기엔 살짝 복잡할 수 있음

### 4. 이상치(Outlier) 처리 (너무 튀는 값 다루기)

- 대부분의 사람들이 50~100kg 인데 500kg값이 있다면?
- 평균도 망가지고, 분석 결과도 엉망이 됨

#### 4-1. 아예 제거하기

- 극단적인 숫자를 가진 값은 분석에서 빼버린다.

```python
Q1 = df['weight'].quantile(0.25)
Q3 = df['weight'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR
df = df[(df['weight'] >= lower) & (df['weight'] <= upper)]
```

- 너무 작거나 너무 큰 값을 '중간 그룹 기준'에서 벗어난 값으로 판단

#### 4-2. 숫자 조절해서 자르기 (Clipping)

- 너무 크거나 작은 숫자를 일정 범위로 잘라서 맞춰요
- Ex: 500kg → 120kg으로 '조정'

```python
df['weight'] = df['weight'].clip(lower=40, upper=120)
```

### 5. 파생 변수 만들기 (Feature Engineering)

- 원래 데이터에서 **새로운 유용한 정보**를 만들어내는 과정

- #### Ex_1:날짜 → 월/요일 추출
  - 날짜 2025-05-06
  - 월(month): 5
  - 요일(weekday): 월요일 (0~6으로 출력됨)

```python
df['date'] = pd.to_datetime(df['date'])
df['month'] = df['date'].dt.month
df['weekday'] = df['date'].dt.weekday
```

- #### Ex_2: 키와 몸무게 → BMI 계산
  - BMI = 몸무게 / (키 x 키)
  - 새롭게 건강 관련 수치(BMI)도 만들 수 있음

```python
df['bmi'] = df['weight'] / (df['height']/100)**2
```

- #### Ex_3: 두 숫자를 곱하거나 나눠보기
  - 매출 = 가격 x 판매량
  - 가격 대비 성능 = 성능 / 가격
  - 위와 같은 방법으로 새로운 변수를 만들 수 있음

### 6. 텍스트 데이터 전처리 다루기 (문장 다듬기)

- 문장(텍스트 데이터)을 이해 못하는 기계 때문에 다듬고 숫자로 변경해야 함

#### 주요 단계

1. **소문자로 바꾸기**: "Hello" → "hello"

2. **불필요한 기호 제거**: "hi!!" → "hi"

3. **단어 나누기 (토큰화)**: "나는 밥을 먹었다" → ["나", "밥", "먹다"]

4. **불필요한 단어 제거**: "는", "을", "가" 같은 조사 제거

5. **벡터로 바꾸기**: 기계가 이해할 수 있도록 숫자로 표현

```python
from sklearn.feature_extraction.text import CountVectorizer
corpus = ["나는 밥을 먹었다", "데이터를 공부하자"]
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
```

---

## 마무리 요약

| 전처리 단계      | 하는 일                          | 쉽게 설명하기                          |
| ---------------- | -------------------------------- | -------------------------------------- |
| 결측치 처리      | 빈칸 채우기 또는 빼기            | 설문조사에서 미응답 처리하기           |
| 범주형 인코딩    | 글자 → 숫자                      | '서울'같은 글자 숫자로 바꾸기          |
| 스케일링         | 숫자 크기 맟추기                 | 몸무게, 나이 등을 같은 기준으로 맟추기 |
| 이상치 처리      | 너무 튀는 값 제거하거나 조절하기 | 실수로 입력한 '500kg' 다듬기           |
| 파생 변수 만들기 | 기존 정보로 새로운 정보 만들기   | 날짜에서 요일 뽑기, BMI 만들기 등      |
| 텍스트 처리      | 문장을 숫자로 바꾸기             | "나는 밥을 먹었다" → 숫자 표현         |

---

---

# t-test란?

- 두 집단이 **"진짜로 다른지"** 확인하는 테스트다.
- 즉, **"그 차이가 우연이 아니라 진짜 차이일까?"** 를 판단하는 도구

## 예시로 이해해보자.

- ### 라면 맛 비교
  - 라면 회사 A와 B가 새로운 국물 맛을 출시함
  - 시식단 10명이 A라면을 먹고, 다른 10명은 B라면을 먹음
  - 점수를 줬더니 평균 점수가:
  - A 라면: 7.1점
  - B 라면: 7.5점

t-test는  
▶ **"이 차이(0.4)는 우연일까?, 진짜 B가 더 맛있는걸까?"** 를 확인하는 것이다.

## t-test의 종류

| 종류             | 언제 쓰나요?                                            |
| ---------------- | ------------------------------------------------------- |
| 독립 표본 t-test | A그룹과 B그룹 **서로 다른 사람들** 비교할 때            |
| 대응 표본 t-test | **같은 사람이 전/후로 측정했을 때**(ex. 다이어트 전/후) |
| 단일표본 t-test  | 어떤 값 하나가 기준보다 다른지 확인할 때                |

## t-test의 기본 구조

### 1. 가설 세우기

- **귀무가설: 두 그룹은 차이가 없다.**
- **대립가설: 두 그룹은 차이가 있다.**

### 2. t값 계산하기

- **두 평균의 차이, 표준편차, 표본 수로 계산**

### 3. p값 확인하기

- 이 결과가 우연히 나올 확률
- - 보통 p값 < 0.05 일 때 → **"우연 아님, 차이 있음!"** 이라고 판단

## 정리

| 용어     | 뜻                                                             |
| -------- | -------------------------------------------------------------- |
| t값      | 평균차이가 **얼마나 큰지** 수치로 표현                         |
| p값      | 그 결과가 **우연일 확률**                                      |
| p < 0.05 | "우연이 아니라 → **진짜 차이가 있는 것 같아!**"(귀무가설 기각) |
| 가설검정 | "두 집단이 진짜 다른가?"를 검토하는 과정                       |
